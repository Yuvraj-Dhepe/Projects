{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Loading the Feature Extractor Backbone\n",
    "import torchvision as tv\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from src.models import backbone_registry as br\n",
    "# Necessary to register the models\n",
    "from src.models import backbones as bb\n",
    "from src.models import anchors\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from src.data import utils as data_utils\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "\n",
    "from src.models import rpn, detector, faster_rcnn\n",
    "DEVICE = \"cuda\"\n",
    "IMAGE_DIR = \"PNGImages\"\n",
    "MASK_DIR = \"PedMasks\"\n",
    "ROOT = \"../../data/raw/PennFudanPed\"\n",
    "\n",
    "torch.manual_seed(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bytes_to_gb(bytes):\n",
    "    gb = bytes / (2**30)\n",
    "    return gb\n",
    "\n",
    "\n",
    "def bytes_to_mb(bytes):\n",
    "    mb = bytes / (2**20)\n",
    "    return mb\n",
    "\n",
    "\n",
    "def torch_memory():\n",
    "    memory_allocated = torch.cuda.memory_allocated()  # in bytes\n",
    "    max_memory_allocated = torch.cuda.max_memory_allocated()  # in bytes\n",
    "\n",
    "    # Convert to GB\n",
    "    memory_allocated_gb = bytes_to_gb(memory_allocated)\n",
    "    max_memory_allocated_gb = bytes_to_gb(max_memory_allocated)\n",
    "\n",
    "    # Convert to MB\n",
    "    memory_allocated_mb = bytes_to_mb(memory_allocated)\n",
    "    max_memory_allocated_mb = bytes_to_mb(max_memory_allocated)\n",
    "\n",
    "    print(\n",
    "        f\"Current GPU memory allocated: {memory_allocated_mb:.2f} MB ({memory_allocated_gb:.2f} GB)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Peak GPU memory allocated: {max_memory_allocated_mb:.2f} MB ({max_memory_allocated_gb:.2f} GB)\"\n",
    "    )\n",
    "\n",
    "\n",
    "def set_torch_mem_limit(fraction=0.3):\n",
    "    # Print total available GPU memory before applying limit\n",
    "    device = torch.cuda.current_device()\n",
    "    prop = torch.cuda.get_device_properties(device)\n",
    "    total_memory = prop.total_memory / 1024**3  # Convert bytes to GB\n",
    "    print(f\"Total available GPU memory: {total_memory:.2f} GB\")\n",
    "\n",
    "    # Set a hard limit on GPU memory usage\n",
    "    torch.cuda.set_per_process_memory_fraction(fraction)\n",
    "\n",
    "    # Print total available GPU memory after applying limit\n",
    "    total_allocated = (\n",
    "        torch.cuda.memory_allocated(device) / 1024**3\n",
    "    )  # Convert bytes to GB\n",
    "    print(\n",
    "        f\"Total allocated GPU memory after setting limit: {total_allocated:.2f} GB\"\n",
    "    )\n",
    "\n",
    "\n",
    "def set_tf_mem_limit(fraction=0.3):\n",
    "    # Print total available GPU memory before applying limit\n",
    "    gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                # Get the GPU device name\n",
    "                print(f\"GPU: {gpu.name}\")\n",
    "\n",
    "                # Print total memory available for each GPU\n",
    "                gpu_memory = tf.config.experimental.get_memory_info(gpu)\n",
    "                total_memory_gb = gpu_memory.total / (1024**3)\n",
    "                print(f\"Total GPU memory: {total_memory_gb:.2f} GB\")\n",
    "\n",
    "                # Set memory limit for each GPU\n",
    "                tf.config.experimental.set_virtual_device_configuration(\n",
    "                    gpu,\n",
    "                    [\n",
    "                        tf.config.experimental.VirtualDeviceConfiguration(\n",
    "                            memory_limit=int(total_memory_gb * fraction)\n",
    "                        )\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                # Print memory limit after setting\n",
    "                print(f\"Memory limit set: {total_memory_gb*fraction:.2f} GB\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.3*24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_torch_mem_limit(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transform = v2.Compose(\n",
    "    [\n",
    "        v2.Resize(\n",
    "            (800, 800), interpolation=v2.functional.InterpolationMode.BICUBIC\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_utils.Masks2BboxDataset(\n",
    "    root=ROOT,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    mask_dir=MASK_DIR,\n",
    "    transforms=resize_transform,\n",
    ")\n",
    "NUM_CLASSES = 1\n",
    "# Get subsets for train, validation, and test splits\n",
    "train_subset = dataset.get_subset(dataset.train_indices)\n",
    "valid_subset = dataset.get_subset(dataset.valid_indices)\n",
    "test_subset = dataset.get_subset(dataset.test_indices)\n",
    "\n",
    "# Create data loaders for train, validation, and test splits\n",
    "train_loader = DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_subset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_subset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda x: tuple(zip(*x)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils.plot_batch(train_loader, batch_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a single Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, targets = zip(*train_loader)\n",
    "single_img = imgs[0][0]\n",
    "print(single_img.shape)\n",
    "batched_single_img = single_img.float().unsqueeze(0).to(\"cuda\")\n",
    "print(batched_single_img.shape)\n",
    "single_targets = targets[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = single_targets[\"masks\"]\n",
    "gt_labels = single_targets[\"labels\"]\n",
    "gt_bboxes = single_targets[\"boxes\"].to(\"cuda\")\n",
    "\n",
    "colors = [mcolors.rgb2hex(plt.get_cmap(\"tab10\")(i)) for i in range(len(masks))]\n",
    "font = \"/usr/share/fonts/truetype/ubuntu/UbuntuMono-R.ttf\"\n",
    "bbox_image = draw_bounding_boxes(\n",
    "    single_img,\n",
    "    gt_bboxes,\n",
    "    gt_labels,\n",
    "    font=font,\n",
    "    width=3,\n",
    "    fill=True,\n",
    "    font_size=30,\n",
    "    colors=colors,\n",
    ")\n",
    "data_utils.plot(bbox_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(gt_bboxes[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = batched_single_img.shape[1:]\n",
    "BACKBONE = \"VGG16\"  # can be \"RESNET34\", \"RESNET50\", \"RESNET101\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb_last_layer_shape, backbone, pf = br.BackBoneRegistry.create_backbone(\n",
    "    BACKBONE, INPUT_SHAPE, pool_to_feature_vector=True\n",
    ")\n",
    "PATCH_SIZE = (\n",
    "    int(INPUT_SHAPE[1] / bb_last_layer_shape[2]),\n",
    "    int(INPUT_SHAPE[2] / bb_last_layer_shape[3]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Last Layer Shape: {bb_last_layer_shape}\")\n",
    "print(\n",
    "    f\"Single pixel in backbone feature map represents a patch of {PATCH_SIZE[0]} X {PATCH_SIZE[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Faster RCNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frcnn_obj = faster_rcnn.FasterRcnn(\n",
    "    num_classes=1,\n",
    "    backbone=backbone,\n",
    "    feature_map_size=bb_last_layer_shape,\n",
    "    pool_to_feature_vector=pf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = batched_single_img.shape[-2:]\n",
    "all_anchor_bboxes, valid_anchor_bbox_indices = anchors.generate_anchor_maps(\n",
    "    feature_map_size=(\n",
    "        bb_last_layer_shape[-2],\n",
    "        bb_last_layer_shape[-1],\n",
    "    ),\n",
    "    image_size=image_shape,\n",
    "    ratios=[0.5, 1, 2],\n",
    "    scales=[8, 16, 32],\n",
    ")\n",
    "all_anchor_bboxes = torch.tensor(all_anchor_bboxes, dtype = torch.float32).to(DEVICE)\n",
    "valid_anchor_bbox_indices = torch.tensor(valid_anchor_bbox_indices, dtype = torch.long).to(DEVICE)\n",
    "print(all_anchor_bboxes.shape, valid_anchor_bbox_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_rpn_map, gt_rpn_labels = anchors.generate_rpn_map(anchor_map = all_anchor_bboxes,valid_anchor_indices = valid_anchor_bbox_indices, gt_bboxes = gt_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values, counts = torch.unique(gt_rpn_labels, return_counts=True)\n",
    "unique_counts = {val.item(): count.item() for val, count in zip(unique_values, counts)}\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import tv_tensors\n",
    "torch_bboxes = tv_tensors.BoundingBoxes(\n",
    "    all_anchor_bboxes, format=\"XYXY\", canvas_size=v2.functional.get_size(single_img)\n",
    ")\n",
    "\n",
    "colors = [\n",
    "    mcolors.rgb2hex(plt.get_cmap(\"tab20\")(i)) for i in range(len(torch_bboxes))\n",
    "]\n",
    "font = \"/usr/share/fonts/truetype/ubuntu/UbuntuMono-R.ttf\"\n",
    "bbox_image = draw_bounding_boxes(\n",
    "    single_img,\n",
    "    torch_bboxes,\n",
    "    font=font,\n",
    "    width=3,\n",
    "    fill=False,\n",
    "    font_size=30,\n",
    "    colors=colors,\n",
    ")\n",
    "data_utils.plot(bbox_image, subplots_kwargs={\"figsize\": (10, 10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coor, y_coor = 400, 400\n",
    "# Subtracting 8 is necessary as we generated bboxes for center of\n",
    "# patches and x_coor and y_coor are the bottomright coordinates of a patch\n",
    "anchrs = anchors.get_anchors_for_coordinate(\n",
    "    all_anchor_bboxes, coordinate=(x_coor - 8, y_coor - 8)\n",
    ")\n",
    "anchrs = [anchor.cpu().numpy() for anchor in anchrs]\n",
    "torch_bboxes = tv_tensors.BoundingBoxes(\n",
    "    anchrs, format=\"XYXY\", canvas_size=v2.functional.get_size(single_img)\n",
    ")\n",
    "\n",
    "colors = [\n",
    "    mcolors.rgb2hex(plt.get_cmap(\"tab20\")(i)) for i in range(len(torch_bboxes))\n",
    "]\n",
    "font = \"/usr/share/fonts/truetype/ubuntu/UbuntuMono-R.ttf\"\n",
    "bbox_image = draw_bounding_boxes(\n",
    "    single_img,\n",
    "    torch_bboxes,\n",
    "    font=font,\n",
    "    width=3,\n",
    "    fill=False,\n",
    "    font_size=30,\n",
    "    colors=colors,\n",
    ")\n",
    "data_utils.plot(bbox_image, subplots_kwargs={\"figsize\": (10, 10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap = backbone.to(DEVICE)(batched_single_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_obj = rpn.RPN(feature_map_size=bb_last_layer_shape, allow_edge_proposals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectness_scores, bbox_deltas_map, rpn_proposals = rpn_obj(\n",
    "    feature_map=fmap,\n",
    "    image_shape=image_shape,\n",
    "    all_anchor_bboxes=all_anchor_bboxes,\n",
    "    valid_anchor_bbox_indices=valid_anchor_bbox_indices,\n",
    "    max_proposals_pre_nms=12000,\n",
    "    max_proposals_post_nms=2000,\n",
    ")\n",
    "print(objectness_scores.shape, bbox_deltas_map.shape, rpn_proposals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_rpn_minibatch_target = faster_rcnn.FasterRcnn._sample_rpn_minibatch(gt_rpn_labels,256,0.5)\n",
    "\n",
    "print(gt_rpn_minibatch_target.shape)\n",
    "unique_values, counts = torch.unique(gt_rpn_labels, return_counts=True)\n",
    "unique_counts = {val.item(): count.item() for val, count in zip(unique_values, counts)}\n",
    "print(unique_counts)\n",
    "\n",
    "unique_values, counts = torch.unique(gt_rpn_minibatch_target, return_counts=True)\n",
    "unique_counts = {val.item(): count.item() for val, count in zip(unique_values, counts)}\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchrs = [anchor.detach().cpu().numpy() for anchor in rpn_proposals.squeeze()]\n",
    "torch_bboxes = tv_tensors.BoundingBoxes(\n",
    "    anchrs, format=\"XYXY\", canvas_size=v2.functional.get_size(single_img)\n",
    ")\n",
    "\n",
    "colors = [\n",
    "    mcolors.rgb2hex(plt.get_cmap(\"tab20\")(i)) for i in range(len(torch_bboxes))\n",
    "]\n",
    "font = \"/usr/share/fonts/truetype/ubuntu/UbuntuMono-R.ttf\"\n",
    "bbox_image = draw_bounding_boxes(\n",
    "    single_img,\n",
    "    torch_bboxes,\n",
    "    font=font,\n",
    "    width=3,\n",
    "    fill=False,\n",
    "    font_size=30,\n",
    "    colors=colors,\n",
    ")\n",
    "data_utils.plot(bbox_image, subplots_kwargs={\"figsize\": (10, 10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals, gt_classes, gt_box_deltas = faster_rcnn.FasterRcnn._label_proposals(proposals=rpn_proposals, gt_boxes=single_targets, min_background_iou_threshold=0.0, min_object_iou_threshold=0.5)\n",
    "\n",
    "proposals, gt_classes, gt_box_deltas = faster_rcnn.FasterRcnn._sample_proposals(\n",
    "            proposals=proposals,\n",
    "            gt_classes=gt_classes,\n",
    "            gt_box_deltas=gt_box_deltas,\n",
    "            max_proposals=128,\n",
    "            positive_fraction=0.25,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_rpn_minibatch_target>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_obj = detector.Detector(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    pool_to_feature_vector=pf,\n",
    "    bb_last_layer_shape=bb_last_layer_shape,\n",
    "    sampling_scale=16.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_classes, detector_box_deltas = detector_obj(feature_map=fmap, proposals=rpn_proposals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_classes.shape, detector_box_deltas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals.shape, gt_classes.shape, gt_box_deltas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals.shape, gt_classes.shape, gt_box_deltas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [mcolors.rgb2hex(plt.get_cmap(\"tab10\")(i)) for i in range(len(gt_classes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_bboxes = tv_tensors.BoundingBoxes(\n",
    "    proposals, format=\"XYXY\", canvas_size=v2.functional.get_size(single_img)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_proposals_bbox_image = draw_bounding_boxes(\n",
    "    single_img,\n",
    "    torch_bboxes,\n",
    "    font=font,\n",
    "    width=3,\n",
    "    fill=False,\n",
    "    font_size=30,\n",
    "    labels=gt_classes,\n",
    "    colors=colors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposals.shape, gt_classes.shape, gt_box_deltas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objectness_scores.shape, gt_rpn_minibatch_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_class_loss = rpn.class_loss(predicted_scores=objectness_scores, objectness_score_target=gt_rpn_minibatch_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_reg_loss = rpn.regression_loss(predicted_box_deltas=bbox_deltas_map, box_deltas_target=gt_rpn_map.unsqueeze(dim = 0), objectness_score_target=gt_rpn_minibatch_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_classes.shape, gt_classes.shape, gt_box_deltas.shape, detector_box_deltas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_class_loss = detector.regression_loss(predicted_box_deltas=detector_classes,y_true=gt_box_deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_class_loss = detector.class_loss(predicted_classes=detector_classes,y_true=gt_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NOTES:\n",
    "  - Don't forget to send the image data and feature extractor to a device\n",
    "  - Also image data should be converted to floats\n",
    "- Feature map size = [1,512,50,50]\n",
    "  - Used in generating anchors # Should be 50, 50\n",
    "  - Used in RPN, for feature map channel count # Should be 1,512,50,50\n",
    "\n",
    "- Image_data:\n",
    "  - Used in FasterRcnn forward pass # Should be 1,3,800,800\n",
    "  - Used in FasterRcnn predict meth # Should be 1,3,800,800\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FasterRcnn:\n",
    "  - all_anchor_bboxes: forward: Bboxes of shape (22500(H*W*A), 4(y1x1y2x2)), A = 9, H,W of feature_map i.e. 50\n",
    "  - anchor_valid_indices: forward: Valid bbox indexes out of all_anchor_bboxes i.e., the ones which don't go outside the image. (len(Valid_indices,),)\n",
    "  - image_data: forward: [bs,C,H,W]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "def plot(df, save_filename, col_name, plot_title):\n",
    "    df = df.dropna()\n",
    "    # Set the style to have only horizontal grid lines\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    sns.set_context(\"paper\", font_scale=1.5, rc={\"lines.linewidth\": 2})\n",
    "\n",
    "    # Generate error values as a percentage of the values (e.g., 10%)\n",
    "    error_percentage = 0.1\n",
    "    errors = df[f\"{col_name}\"] * error_percentage\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Create the bar plot with Seaborn\n",
    "    sns.barplot(\n",
    "        x=df[\"Sample_number\"],\n",
    "        y=df[f\"{col_name}\"],\n",
    "        ax=ax,\n",
    "        errorbar=None,\n",
    "        edgecolor=\"black\",\n",
    "        color=\"#0099ff\",\n",
    "        width=0.6,  # Adjust the width of the bars here\n",
    "    )\n",
    "\n",
    "    # Add error bars\n",
    "    ax.errorbar(\n",
    "        df[\"Sample_number\"],\n",
    "        df[f\"{col_name}\"],\n",
    "        yerr=errors,\n",
    "        fmt=\"none\",\n",
    "        c=\"black\",\n",
    "        capsize=5,\n",
    "    )\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(\"Sample number\")\n",
    "    ax.set_ylabel(\"DNA copies per gms\")\n",
    "    ax.set_title(f\"{plot_title}\")\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Format y-axis in scientific notation with custom format\n",
    "    formatter = FuncFormatter(lambda x, _: f\"{x:.2e}\")\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    # Show only horizontal grid lines\n",
    "    ax.grid(True, which=\"major\", axis=\"y\", linestyle=\"--\", linewidth=0.5)\n",
    "    ax.grid(False, which=\"major\", axis=\"x\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_filename, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../rough/Average_AUBAC.csv\")\n",
    "# Call the function with the DataFrame and desired save filename\n",
    "plot(df, \"EUBAC_plot.png\", col_name=\"Avg_EUBAC\", plot_title=\"EUBAC Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../rough/Average_DSBR.csv\")\n",
    "# Call the function with the DataFrame and desired save filename\n",
    "plot(df, \"DSBR_plot.png\", \"Avg_DSBR\", \"DSBR Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../rough/Average_MCRA.csv\")\n",
    "# Call the function with the DataFrame and desired save filename\n",
    "plot(df, \"MCRA_plot.png\", \"Avg_MCRA\", \"MCRA Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../rough/Average_PMOA621.csv\")\n",
    "# Call the function with the DataFrame and desired save filename\n",
    "plot(df, \"PMOA621_plot.png\", \"Avg_PMOA621\", \"PMOA621 Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../rough/Average_PMOA661.csv\")\n",
    "# Call the function with the DataFrame and desired save filename\n",
    "plot(df, \"PMOA661_plot.png\", \"Avg_PMOA661\", \"PMOA661 Plot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
