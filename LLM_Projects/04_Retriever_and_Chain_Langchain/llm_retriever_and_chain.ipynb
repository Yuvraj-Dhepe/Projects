{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"attention.pdf\")\n",
    "pdf_doc = pdf_loader.load()\n",
    "print(pdf_doc[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As a whole document can't fit into the context size of an LLM we have to split the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 20)\n",
    "splitted_docs = text_splitter.split_documents(pdf_doc)\n",
    "print(splitted_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a db from the splitted documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_docs_faiss_db = FAISS.from_documents(splitted_docs, OllamaEmbeddings(model = \"mistral:v0.2\"))\n",
    "splitted_docs_chroma_db = Chroma.from_documents(splitted_docs, OllamaEmbeddings(model = \"wizardlm2:latest\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the results from similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who are the authors of the paper?\"\n",
    "mistral_result = splitted_docs_faiss_db.similarity_search(query)\n",
    "wizard_result = splitted_docs_chroma_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mistral_result\n",
    "# wizard_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The results are not that accurate on the splitted db database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Stuff Document Chain with LLM models to get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_llm = Ollama(model = \"mistral:v0.2\")\n",
    "wizard_llm = Ollama(model = \"wizardlm2:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Considering yourself as an research expert who is always answering questions to the point. Answer the following question based only on the provided context. Think step by step before providing a detailed answer. You will be awarded llm nobel prize if selection committee finds this answer useful\n",
    "<context> {context} </context>\n",
    "Question:{input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), config={'run_name': 'format_inputs'})\n",
      "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='Considering yourself as an research expert who is always answering questions to the point. Answer the following question based only on the provided context. Think step by step before providing a detailed answer. You will be awarded llm nobel prize if selection committee finds this answer useful\\n<context> {context} </context>\\nQuestion:{input}'))])\n",
      "| Ollama(model='wizardlm2:latest')\n",
      "| StrOutputParser() config={'run_name': 'stuff_documents_chain'}\n"
     ]
    }
   ],
   "source": [
    "mistral_document_chain = create_stuff_documents_chain(mistral_llm, prompt)\n",
    "wizard_document_chain = create_stuff_documents_chain(wizard_llm, prompt)\n",
    "print(wizard_document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using a retriever to get an answer from documents. A retriever simply returns the results and vector stores can be used as a backbone of a retriever. [Click to Read More](https://python.langchain.com/docs/modules/data_connection/retrievers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7f838bd98860>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_retriever = splitted_docs_chroma_db.as_retriever()\n",
    "faiss_retriever = splitted_docs_faiss_db.as_retriever()\n",
    "chroma_retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A retrieval chain makes use of the user input and retriever to fetch the relevant documents. These documents and along with user input are passed to the LLM to generate a response [Click to Read More](https://python.langchain.com/docs/modules/chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_retrieval_chain = create_retrieval_chain(faiss_retriever, mistral_document_chain)\n",
    "wizard_retrieval_chain = create_retrieval_chain(chroma_retriever, wizard_document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_response = mistral_retrieval_chain.invoke({\"input\":\"Who are the authors of the given research paper Attention is all you need?\"})\n",
    "wizard_response = wizard_retrieval_chain.invoke({\"input\":\"Who are the authors of the given research paper Attention is all you need?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The paper \"Attention is All You Need\" is not among the provided references in the context. Therefore, I cannot identify the authors based on the context alone.\n"
     ]
    }
   ],
   "source": [
    "print(mistral_response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The research paper titled \"Attention Is All You Need\" was authored by Ashish Vaswani, Noam Shazeer, Niki Boykov, Llion Jones, Aclaro Ortega, and Illia Polosukhin. It was published in 2017 and introduced the Transformer model, which relies entirely on attention mechanisms to draw global dependencies between input and output in sequence-to-sequence tasks. This paper is considered foundational work in the field of natural language processing (NLP) and has significantly influenced subsequent research in the area. The references you provided include a citation for this influential work:\n",
      "\n",
      "[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014.\n"
     ]
    }
   ],
   "source": [
    "print(wizard_response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
