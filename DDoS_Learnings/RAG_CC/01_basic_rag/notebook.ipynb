{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af29a687",
   "metadata": {},
   "source": [
    "## Library Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ed381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60323b",
   "metadata": {},
   "source": [
    "After 2 years of reading and testing every ğ˜ğ—¶ğ—ºğ—² ğ˜€ğ—²ğ—¿ğ—¶ğ—²ğ˜€ ğ—³ğ—¼ğ˜‚ğ—»ğ—±ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—ºğ—¼ğ—±ğ—²ğ—¹, my conclusion is this:\n",
    "\n",
    "â¡ï¸ ğ——ğ—²ğ—°ğ—¼ğ—±ğ—²ğ—¿-ğ—¼ğ—»ğ—¹ğ˜† models lead in forecasting.\n",
    "\n",
    "â¡ï¸ ğ—˜ğ—»ğ—°ğ—¼ğ—±ğ—²ğ—¿ ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€ work better for \"time series understanding\" tasksâ€”e.g. imputation, anomaly detection.\n",
    "\n",
    "â¡ï¸ ğ—˜ğ—»ğ—°ğ—¼ğ—±ğ—²ğ—¿-ğ——ğ—²ğ—°ğ—¼ğ—±ğ—²ğ—¿ ğ—ºğ—¼ğ—±ğ—²ğ—¹ğ˜€ (e.g. Chronos) remain underexplored. TimeGPT is likely one.\n",
    "\n",
    "This mirrors NLP: encoders for supervised tasks like text classification, decoders for text generation.\n",
    "\n",
    "Btw, a remarkable forecasting model is Toto. Tutorials in the comments! ğŸ‘‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28c948",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")\n",
    "QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", 6333))\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_HOST\", \"localhost\")\n",
    "OLLAMA_PORT = int(os.getenv(\"OLLAMA_PORT\", 11434))\n",
    "DATA_DIR = \"../docs\"\n",
    "REQUIRED_EXTS = [\".pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bf12f",
   "metadata": {},
   "source": [
    "## Setup the Qdrant vector DB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd0eee8",
   "metadata": {},
   "source": [
    "- We create a collection in which we will store all the vector embeddings\n",
    "- These vector embeddings will be indexed for efficient search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0402fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name = \"rag_cc\"\n",
    "client = qdrant_client.QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08565489",
   "metadata": {},
   "source": [
    "### Read the documents from a DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e088f",
   "metadata": {},
   "source": [
    "- Loading the data through llama directory reader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6265bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = DATA_DIR\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=DATA_DIR, required_exts=REQUIRED_EXTS, recursive=True\n",
    ")\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c771bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc60e6",
   "metadata": {},
   "source": [
    "## Create an Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa911a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "\n",
    "def create_index(documents):\n",
    "    # Create a QdrantVectorStore instance\n",
    "    vector_store = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    # Configure storage settings by specifying the vector store as the storage backend\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Create an index by embedding each document and storing it in the vector store\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents=documents, storage_context=storage_context\n",
    "    )\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74718db2",
   "metadata": {},
   "source": [
    "### Load the embedding model and index the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c708c253",
   "metadata": {},
   "source": [
    "- Even though the process is not visible, this is what happens under the hood:\n",
    "    - The documents are chunked using a chunking method\n",
    "    - After chunking the embedding model is used to create embeddings of the data \n",
    "    - Once we have embeddings, they are indexed and stored in the vector store\n",
    "- Later we can fetch the embeddings based on similarity scores to user queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d8f86e",
   "metadata": {},
   "source": [
    "- **We need the Qdrant container to be running for the indexing to work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b427988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"qllama/multilingual-e5-small:latest\",\n",
    "    base_url=OLLAMA_BASE_URL,\n",
    ")\n",
    "# Add the embedding model to the settings, to be used by the index creation process\n",
    "Settings.embed_model = embed_model\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674eae0",
   "metadata": {},
   "source": [
    "### Load the LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94980306",
   "metadata": {},
   "source": [
    "- After we have created our vector database, we will now use LLMs, which will use user query, and relevant context to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f98018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma3n:e2b\", base_url=OLLAMA_BASE_URL, request_timeout=60)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0bfd15",
   "metadata": {},
   "source": [
    "### Define the Prompt Template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27581aa5",
   "metadata": {},
   "source": [
    "- We use a prompt template, for the LLM to generate a response based on the query and the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2afbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5784d4",
   "metadata": {},
   "source": [
    "### Reranking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edfc3b5",
   "metadata": {},
   "source": [
    "- Based on the user query, the query engine will return us the top_k most similar contexts to the query\n",
    "- To fine-grain the contexts more we use a reranker model\n",
    "- Reranker is a sophisticated model (often a cross-encoder) which evaluates the initial list of retrieved chunks alongside the query to assign a relevance score to each chunk, and we pick top_n context chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054482c4",
   "metadata": {},
   "source": [
    "### Query the Document "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef9a23",
   "metadata": {},
   "source": [
    "- The query engine integrates the retrieval, re-ranking, and prompt based response generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233cbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f96a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What is this pdf about? Answer only based on the content of the file and not the file path. Remember DDoS stands for Daily Dose of Data Science not Distributed Denial of Service Attacks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d82f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5d53c",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c5626f",
   "metadata": {},
   "source": [
    "- The end to end process of RAG is:\n",
    "    - Document Chunking\n",
    "    - Embedding the chunks into vectors (Encoder of the whole process)\n",
    "    - Indexing the vectors\n",
    "    - Storing vectors in the Vector DB\n",
    "    - Converting user query into vector using same model \n",
    "    - Querying vector DB to find ANNs to user query\n",
    "    - Retrieval of similar vector chunks to that of query \n",
    "    - Re-ranking the retrieved chunks \n",
    "    - Feeding the top_n chunks as context along with query to the LLM (Decoder of the whole process)\n",
    "    - Generating response for the user query with relevant context "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebd766b",
   "metadata": {},
   "source": [
    "### A bit of maths "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490a32b4",
   "metadata": {},
   "source": [
    "- For getting the vector similarity we use: \n",
    "    - Cosine Similarity (Cos(theta)) as it measures the cosine of the angle between two vectors, providing a metric for their orientation rather than magnitude. \n",
    "    - It ranges from -1 to 1, where 0 means no similarity, -1 indicates high dissimilarity and 1 indicates high similarity\n",
    "    - It's better than using Dot product (ABCos(theta)) as it removes the influence of magnitude when comparing vectors, so if there are 2 vectors, one having high magnitude, while the other not, the Dot products' value will be large (in both positive / negative way depending on the angle, so this is an influenced measure, but when the influence is normalized by removing the magnitudes we get the Cosine similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_CC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
