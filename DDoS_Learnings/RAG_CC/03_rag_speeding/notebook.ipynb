{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feeb1434",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import os\n",
    "import nest_asyncio\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "nest_asyncio.apply()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8769bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")\n",
    "QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", 6333))\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(data):\n",
    "    if isinstance(data, str):\n",
    "        display(Markdown(data))\n",
    "    elif isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            display(Markdown(f\"**{key}:** {value}\"))\n",
    "    else:\n",
    "        display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d35c3b",
   "metadata": {},
   "source": [
    "## Traditional RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfdbfe",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b248cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "## Step 1: Load the SQuAD dataset\n",
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "## Step 2 : Extract unique contexts from the dataset\n",
    "data = [item[\"context\"] for item in dataset[\"train\"]]\n",
    "texts = list(set(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953ec00",
   "metadata": {},
   "source": [
    "### Embed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ffc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def batch_iterate(lst, batch_size):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i : i + batch_size]\n",
    "\n",
    "\n",
    "class EmbedData:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_model_name=\"hf.co/Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0\",\n",
    "        batch_size=32,\n",
    "    ):\n",
    "        self.embed_model_name = embed_model_name\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_model = self._load_embed_model()\n",
    "        self.embeddings = []\n",
    "\n",
    "    def _load_embed_model(self):\n",
    "        embed_model = OllamaEmbedding(\n",
    "            model_name=self.embed_model_name,\n",
    "            base_url=OLLAMA_BASE_URL,\n",
    "        )\n",
    "        return embed_model\n",
    "\n",
    "    def generate_embeddings(self, text):\n",
    "        self.embeddings = self.embed_model.get_text_embedding_batch(texts=text)\n",
    "        return self.embeddings\n",
    "\n",
    "    def embed(self, contexts):\n",
    "        self.contexts = contexts\n",
    "        for batch in tqdm(batch_iterate(contexts, self.batch_size), desc=\"Embedding\"):\n",
    "            embeddings = self.generate_embeddings(batch)\n",
    "            self.embeddings.extend(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e99afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "embeddata = EmbedData(batch_size=batch_size)\n",
    "embeddata.embed(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e6a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(embeddata.contexts[0]), pretty_print(embeddata.embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa25e11",
   "metadata": {},
   "source": [
    "### Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500b3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "\n",
    "class QdrantVDB:\n",
    "    def __init__(self, collection_name, vector_dim=768, batch_size=512):\n",
    "        self.collection_name = collection_name\n",
    "        self.vector_dim = vector_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def define_client(self):\n",
    "        self.client = QdrantClient(\n",
    "            url=f\"http://{QDRANT_HOST}:{QDRANT_PORT}\",\n",
    "            prefer_grpc=False,\n",
    "        )\n",
    "\n",
    "    def create_collection(self):\n",
    "        if not self.client.collection_exists(self.collection_name):\n",
    "            self.client.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                # NOTE: We use similarity search with dot product, and store the vectors\n",
    "                # on disk instead of memory to optimize memory usage for large datasets\n",
    "                vectors_config=models.VectorParams(\n",
    "                    size=self.vector_dim, distance=models.Distance.DOT, on_disk=True\n",
    "                ),\n",
    "                # NOTE: Optimizer config is necessary to optimize storage\n",
    "                # and indexing performance\n",
    "                optimizers_config=models.OptimizersConfigDiff(\n",
    "                    default_segment_number=9, indexing_threshold=0\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def ingest_data(self, embeddata):\n",
    "        # Zip the contexts and embeddings into pairs (eagerly convert to list for len())\n",
    "        paired_data = list(zip(embeddata.contexts, embeddata.embeddings))\n",
    "        # Iterate over zipped batches of (context, embedding) pairs\n",
    "        for batch in tqdm(\n",
    "            batch_iterate(paired_data, self.batch_size),\n",
    "            total=len(paired_data) // self.batch_size,\n",
    "            desc=\"Ingesting in batches\",\n",
    "        ):\n",
    "            # Unzip the batch into separate lists of contexts and embeddings\n",
    "            batch_contexts, batch_embeddings = zip(*batch)\n",
    "\n",
    "            # Upload the batch to the collection\n",
    "            # For each batch, we invoke the .client.upload_collection to store the embeddings and their associated metadata (payload). Payload stores metadata such as the original context for each vector.\n",
    "\n",
    "            self.client.upload_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                vectors=batch_embeddings,  # List of embedding vectors\n",
    "                payload=[\n",
    "                    {\"context\": context} for context in batch_contexts\n",
    "                ],  # Associated metadata\n",
    "            )\n",
    "\n",
    "        # Configuration to update the collection only if the total data ingested in the latest run exceeds a certain threshold.\n",
    "        # We specify the threshold, so that we are not updating the vector db as soon as a new entry is added, but rather after a certain number of entries have been added.\n",
    "        self.client.update_collection(\n",
    "            collection_name=self.collection_name,\n",
    "            optimizer_config=models.OptimizersConfigDiff(indexing_threshold=20000),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d3346",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = QdrantVDB(collection_name=\"squad_collection\")\n",
    "database.define_client()\n",
    "database.create_collection()\n",
    "database.ingest_data(embeddata)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd9e1e",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a7466a",
   "metadata": {},
   "source": [
    "- Encapsulate the logic for searching the vector db using a query (of string type)\n",
    "- Using the embedding model and vector db, we can retriee the most relevant contexts based on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdaf28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, vector_db, embeddata):\n",
    "        self.vector_db = vector_db\n",
    "        self.embeddata = embeddata\n",
    "\n",
    "    def search(self, query):\n",
    "        # Use hf function to get the query embedding\n",
    "        query_embedding = self.embeddata.embed_model.get_query_embedding(query)\n",
    "\n",
    "        #\n",
    "        start_time = time.time()\n",
    "        result = self.vector_db.client.query_points(\n",
    "            collection_name=self.vector_db.collection_name,\n",
    "            query=query_embedding,\n",
    "            search_params=models.SearchParams(\n",
    "                quantization=models.QuantizationSearchParams(\n",
    "                    ignore=True, rescore=True, oversampling=2.0\n",
    "                )\n",
    "                # Ignore quantization during search for high precision\n",
    "                # Rescore the results after the initial quantized search for better accuracy\n",
    "                # Oversampling to fetch additional candidates to improve result quality\n",
    "            ),\n",
    "            timeout=1000,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        print(f\"Execution time for search: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11058b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Retriever(database, embeddata).search(\"Sample Query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56671a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in result.points:\n",
    "    pretty_print(dict(data)[\"payload\"][\"context\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df90e56",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d899b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, retriever, llm_name=\"phi3:3.8b\"):\n",
    "        self.llm_name = llm_name\n",
    "        self.llm = self._setup_llm()\n",
    "        self.retriever = retriever\n",
    "        self.qa_prompt_tmpl_str = \"\"\"Context information is below.\n",
    "                                     ---------------------\n",
    "                                     {context}\n",
    "                                     ---------------------\n",
    "                                     \n",
    "                                     Given the context information above I want you\n",
    "                                     to think step by step to answer the query in a\n",
    "                                     crisp manner, incase case you don't know the\n",
    "                                     answer say 'I don't know!'\n",
    "                                     \n",
    "                                     ---------------------\n",
    "                                     Query: {query}\n",
    "                                     ---------------------\n",
    "                                     Answer: \"\"\"\n",
    "\n",
    "    def _setup_llm(self):\n",
    "        return Ollama(model=self.llm_name, base_url=OLLAMA_BASE_URL)\n",
    "\n",
    "    # Retrieve relevant results from the vector database\n",
    "    def generate_context(self, query):\n",
    "        # Use the retriever to get relevant context\n",
    "        search_result = self.retriever.search(query)\n",
    "        if not search_result.points:\n",
    "            return \"No relevant context found.\"\n",
    "\n",
    "        # Iterate through the search results and extract the context field from\n",
    "        # each points payload and append each context to a list called combined_prompt\n",
    "        context = [dict(point) for point in search_result.points]\n",
    "        combined_prompt = []\n",
    "        for entry in context:\n",
    "            context = entry[\"payload\"][\"context\"]\n",
    "            combined_prompt.append(context)\n",
    "\n",
    "        return \"\\n\\n --- \\n\\n\".join(combined_prompt)\n",
    "\n",
    "    # Collating everything together into a query method, which will accept the user query,\n",
    "    # generate a context for it, format the prompt template, to create a prompt, send it to the LLM, and return the generated response.\n",
    "\n",
    "    def query(self, query):\n",
    "        context = self.generate_context(query)\n",
    "\n",
    "        prompt = self.qa_prompt_tmpl_str.format(context=context, query=query)\n",
    "\n",
    "        response = self.llm.complete(prompt)\n",
    "\n",
    "        return context, dict(response)[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b69c0",
   "metadata": {},
   "source": [
    "### Using RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667eece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = Retriever(database, embeddata)\n",
    "rag = RAG(retriever, llm_name=\"phi3:3.8b\")\n",
    "\n",
    "\n",
    "# Taking a look at dummy data, and forming a query based on it\n",
    "pretty_print(embeddata.contexts[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"The premium and VIP services in Airports are reserved for which type of passengers?\"\n",
    "context, response = rag.query(query)\n",
    "\n",
    "pretty_print(context), pretty_print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_CC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
