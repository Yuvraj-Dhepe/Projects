{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db860601",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ed381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60323b",
   "metadata": {},
   "source": [
    "After 2 years of reading and testing every ùòÅùó∂ùó∫ùó≤ ùòÄùó≤ùóøùó∂ùó≤ùòÄ ùó≥ùóºùòÇùóªùó±ùóÆùòÅùó∂ùóºùóª ùó∫ùóºùó±ùó≤ùóπ, my conclusion is this:\n",
    "\n",
    "‚û°Ô∏è ùóóùó≤ùó∞ùóºùó±ùó≤ùóø-ùóºùóªùóπùòÜ models lead in forecasting.\n",
    "\n",
    "‚û°Ô∏è ùóòùóªùó∞ùóºùó±ùó≤ùóø ùó∫ùóºùó±ùó≤ùóπùòÄ work better for \"time series understanding\" tasks‚Äîe.g. imputation, anomaly detection.\n",
    "\n",
    "‚û°Ô∏è ùóòùóªùó∞ùóºùó±ùó≤ùóø-ùóóùó≤ùó∞ùóºùó±ùó≤ùóø ùó∫ùóºùó±ùó≤ùóπùòÄ (e.g. Chronos) remain underexplored. TimeGPT is likely one.\n",
    "\n",
    "This mirrors NLP: encoders for supervised tasks like text classification, decoders for text generation.\n",
    "\n",
    "Btw, a remarkable forecasting model is Toto. Tutorials in the comments! üëá"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28c948",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")\n",
    "QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", 6333))\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_HOST\", \"localhost\")\n",
    "OLLAMA_PORT = int(os.getenv(\"OLLAMA_PORT\", 11434))\n",
    "DATA_DIR = \"../docs\"\n",
    "REQUIRED_EXTS = [\".txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bf12f",
   "metadata": {},
   "source": [
    "## Setup the Qdrant vector DB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a8dd2",
   "metadata": {},
   "source": [
    "- We create a collection in which we will store all the vector embeddings\n",
    "- These vector embeddings will be indexed for efficient search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0402fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name = \"rag_cc\"\n",
    "client = qdrant_client.QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08565489",
   "metadata": {},
   "source": [
    "### Read the documents from a DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1375c9",
   "metadata": {},
   "source": [
    "- Loading the data through llama directory reader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6265bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = DATA_DIR\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=DATA_DIR, required_exts=REQUIRED_EXTS, recursive=True\n",
    ")\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c771bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc60e6",
   "metadata": {},
   "source": [
    "## Create an Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa911a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "\n",
    "def create_index(documents):\n",
    "    # Create a QdrantVectorStore instance\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "\n",
    "    # Configure storage settings by specifying the vector store as the storage backend\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Create an index by embedding each document and storing it in the vector store\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents=documents, storage_context=storage_context\n",
    "    )\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74718db2",
   "metadata": {},
   "source": [
    "### Load the embedding model and index the data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e221e7d",
   "metadata": {},
   "source": [
    "- Even though the process is not visible, this is what happens under the hood:\n",
    "    - The documents are chunked using a chunking method\n",
    "    - After chunking the embedding model is used to create embeddings of the data \n",
    "    - Once we have embeddings, they are indexed and stored in the vector store\n",
    "- Later we can fetch the embeddings based on similarity scores to user queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e59f7c0",
   "metadata": {},
   "source": [
    "- **We need the Qdrant container to be running for the indexing to work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b427988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = FastEmbedEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    ")\n",
    "# Add the embedding model to the settings, to be used by the index creation process\n",
    "Settings.embed_model = embed_model\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674eae0",
   "metadata": {},
   "source": [
    "### Load the LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1256f753",
   "metadata": {},
   "source": [
    "- After we have created our vector database, we will now use LLMs, which will use user query, and relevant context to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f98018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma3n:e2b\", base_url=OLLAMA_BASE_URL, request_timeout=60)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0bfd15",
   "metadata": {},
   "source": [
    "### Define the Prompt Template "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19555b56",
   "metadata": {},
   "source": [
    "- We use a prompt template, for the LLM to generate a response based on the query and the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2afbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5784d4",
   "metadata": {},
   "source": [
    "### Reranking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff5f1b",
   "metadata": {},
   "source": [
    "- Based on the user query, the query engine will return us the top_k most similar contexts to the query\n",
    "- To fine-grain the contexts more we use a reranker model\n",
    "- Reranker is a sophisticated model (often a cross-encoder) which evaluates the initial list of retrieved chunks alongside the query to assign a relevance score to each chunk, and we pick top_n context chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054482c4",
   "metadata": {},
   "source": [
    "### Query the Document "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef9a23",
   "metadata": {},
   "source": [
    "- The query engine integrates the retrieval, re-ranking, and prompt based response generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233cbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f96a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"\"\"How did the structure of funding startups in batches contribute to the success and growth of the Y Combinator program and the startups involved?\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d82f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0c2bf",
   "metadata": {},
   "source": [
    "\n",
    "## Generating Evaluation Dataset using Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39a3c7",
   "metadata": {},
   "source": [
    "- Relying on RAG systems with gut feelings is not the way to go. \n",
    "- It's better to have evaluations on the way, to see what works and what doesn't in reality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57fa4c0",
   "metadata": {},
   "source": [
    "- Chunking might not be precise and useful.\n",
    "- The retrieval model might not always fetch the most relevant document.\n",
    "- The generative model might misinterpret the context, leading to inaccurate or misleading answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41b9df",
   "metadata": {},
   "source": [
    "### Load the knowledge Base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c11718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3209dfed",
   "metadata": {},
   "source": [
    "- In RAG the availability of ground truths might not be available as the RAG can be applied to a very specific domain, so referene truths will be tricky to obtain \n",
    "- Hence we evaluate the RAG systems using the reference-free metrics that capture the **quality** of the generated response which is what precisely matters in the rag application. \n",
    "- These metrics rely on: \n",
    "    - question (q)\n",
    "    - retrieved context (c(q))\n",
    "    - generated response/answer (a(q)) \n",
    "\n",
    "- We look at the following metrics here: \n",
    "    - Faithfullness: Is the generated response (a(q)) faithful to the retrieved context c(q)?\n",
    "        - A high faithfullness score means the generated text uses **ONLY** tthe information provided in the retrieved documents without irrelevant or hallucinations\n",
    "    \n",
    "    - Answer Relevance: Is the generated response (a(q)) relevant to the user query in meaningful and complete way ?\n",
    "        - A high score means the response fully covers the users intent providing the information that is specific to the question asked. This metric discourages responses which may be technically correct but are either too broad, partially off-topic, or contain unnecessary information. \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    \"../docs/paul_graham\",\n",
    ")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "documents = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff42fd",
   "metadata": {},
   "source": [
    "### Setting up Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2058a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "generator_llm = ChatOllama(model=\"phi3:3.8b\", base_url=OLLAMA_BASE_URL)\n",
    "critic_llm = ChatOllama(model=\"llama3.2:1b\", base_url=OLLAMA_BASE_URL)\n",
    "ollama_emb = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=OLLAMA_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79a97d",
   "metadata": {},
   "source": [
    "### Creating Ragas Testset Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "# dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602cc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3bae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, ollama_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e96547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILS, becaus of some dependency issues with langchain\n",
    "# distribution = {\"simple\":0.6, \"reasoning\":0.3, \"multi_context\":0.245}\n",
    "# testset = generator.generate_with_langchain_docs(documents, testset_size=10, query_distribution=distribution, raise_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbab1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testset from a file\n",
    "# test_df = testset.to_pandas().dropna()\n",
    "test_df = pd.read_csv(\"../docs/paul_graham/test_data_paul_graham.csv\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7597db4",
   "metadata": {},
   "source": [
    "- Below function that will accept the query engine and a question, and return the answer along with the context it looked at to generate the corresponding answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c170c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query_engine, question):\n",
    "    response = query_engine.query(question)\n",
    "    return {\n",
    "        \"answer\": response.response,\n",
    "        \"context\": [c.node.get_content() for c in response.source_nodes],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "test_questions = test_df[\"question\"].values\n",
    "\n",
    "responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19134a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, response in enumerate(responses):\n",
    "    print(response.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be51113",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": [response[\"answer\"] for response in responses],\n",
    "    \"contexts\": [response[\"context\"] for response in responses],\n",
    "    \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n",
    "}\n",
    "\n",
    "ragas_eval_dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f095a93",
   "metadata": {},
   "source": [
    "## Metric Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d58cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ee0eb",
   "metadata": {},
   "source": [
    "### Faithfullness\n",
    "- $$F = \\frac{V}{S}$$\n",
    "    - S is total number of assertive statements generated from a(q) which make a specific claim in the response. \n",
    "    - V are the total number of statements from S that can be verified against the retrieved context.\n",
    "\n",
    "- Goal: considering the retrieved documents as source of truth, how trustworthy are the the LLM responses\n",
    "    - A high faithfulness score indicates that most or all statements in the answer are verifiable within the context, meaning the answer closely aligns with the information provided by the retrieval engine.\n",
    "    - F evaluates the **quality of the answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19543f",
   "metadata": {},
   "source": [
    "### Answer Relevance\n",
    "- $$AR = \\frac{1}{n} \\sum_{i=1}^{n} cos_sim(q, q_i)$$\n",
    "    - $q$ is the original question \n",
    "    - $q_i$ is a generated question from the answer \n",
    "\n",
    "- Goal: Considering the generated answer by LLM as a true answer, how well is the **answer aligned to the original question**, as it can match a variety of questions reflecting the same intent. \n",
    "    - A high AR score indicates the answer is well-aligned to the original question. \n",
    "    - AR evaluates **the quality of the answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc7c5f",
   "metadata": {},
   "source": [
    "### Context Relevance\n",
    "- $$ CR = \\frac{Number\\ of\\ extracted\\ sentences}{Total\\ Number\\ of\\ Sentences\\ in\\ c(q)} $$\n",
    "    - Here the sentences refer to all sentences retrieved from the vector DB\n",
    "    - The numerator represents only the relevant sentences required to answer the question\n",
    "- Goal: Considering the LLM answer is true, how relevant was the context to give that answer\n",
    "    - A high score means the extracted context is highly relevant for generating the specific answer.\n",
    "    - CR evaluates the **quality of the context**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a35ee5",
   "metadata": {},
   "source": [
    "### Answer Correctness\n",
    "- $$ FAC = \\frac{TP}{(TP + 0.5 * (FP + FN))}$$\n",
    "    - TP: Statements that are present in both the answer and ground truth \n",
    "    - FP: Statements that are present in answer but not found in the ground truth\n",
    "    - FN: Statements that are not present in the answer but are present in the ground truth \n",
    "- Goal: Check answers' correctness both factually & semantically\n",
    "    - **Ground truth is a requirement** for this metric.\n",
    "    - **A critic LLM** will check the factual correctness by comparing the generated answer and the gt \n",
    "    - An embedding model compoutes the embeddings for the generated answer and the gt and then measures the cosine of the angle between the 2 embeddings, which helps to determine the cosine similarity\n",
    "    - FAC evaluates **the quality of answer wrt ground truth**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc0ce2",
   "metadata": {},
   "source": [
    "### Context Recall \n",
    "- $$ \\text{Context Recall} = \\frac{\\text{Number of sentences that can be attributed to context}}{\\text{Number of sentences in GT}}$$\n",
    "    - Sentences are sentences from the ground_truth \n",
    "\n",
    "- Goal: To check how much of the retrieved context aligns with the ground_truth answer \n",
    "    - **Ground truth is a requirement** for this metric.\n",
    "    - **A critic LLM** judges how much of the retrieved context aligns with the ground truth answer\n",
    "    - CR evaluates the **quality of the retrieved context wrt ground truth**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d29bafa",
   "metadata": {},
   "source": [
    "### Context Precision\n",
    "- Context precision measures **if all relevant items in the contexts are ranked high or not**. \n",
    "    - It checks how precise the fetched context is.\n",
    "- Goal: Check the preciseness of the fetched content\n",
    "    - Given the question, the ground truth answer, and the retrieved context, verify if the context was useful in arriving at the given answer.\n",
    "    - **Ground Truth & Critic LLM are a requirement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef110e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [faithfulness, answer_correctness, context_recall, context_precision]\n",
    "\n",
    "evaluation_result = evaluate(\n",
    "    llm=critic_llm, embeddings=ollama_emb, dataset=ragas_eval_dataset, metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a147787",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores_df = pd.DataFrame(evaluation_result.scores)\n",
    "eval_scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4fb65",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- The evaluation process involves majorly checking the quality of the retrieved context & generated answer via different metrics. \n",
    "- All metrics are self-contained/reference-free. \n",
    "    - Evaluation **without GT**:\n",
    "        - Faithfullness: Evaluates the **quality of answer**, assuming the **retrieved context is correct**\n",
    "        - Answer Relevance: Evaluates the **quality of answeer**, via use of semantic similarity of **generated questions** wrt original question\n",
    "        - Context Relevance:Evaluates the **quality of context**, assuming **generated answer is correct** via use of **critic LLM** to know how many of the sentences in retrieved context would be necessary to come up with the generated answer\n",
    "    \n",
    "    - Evaluation **wrt GT**: \n",
    "        - Answer Correctness: Evaluates the **quality of answer** by verifying it's **factuallness via critc LLM** as well as **semantic similarity** to the **ground truth**\n",
    "        - Context Recall: Evaluates the **quality of context** by verifying the sentences from **ground_truth, which can be based upon the retrieved context (via critic LLM)** \n",
    "        - Context Precision: Evaluates the **quality of context**  by noting via **critic llm** how precise is the fetched content to come up with the GT for the given question\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_CC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
