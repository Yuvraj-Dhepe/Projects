{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db860601",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ed381",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "import os\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da60323b",
   "metadata": {},
   "source": [
    "After 2 years of reading and testing every 𝘁𝗶𝗺𝗲 𝘀𝗲𝗿𝗶𝗲𝘀 𝗳𝗼𝘂𝗻𝗱𝗮𝘁𝗶𝗼𝗻 𝗺𝗼𝗱𝗲𝗹, my conclusion is this:\n",
    "\n",
    "➡️ 𝗗𝗲𝗰𝗼𝗱𝗲𝗿-𝗼𝗻𝗹𝘆 models lead in forecasting.\n",
    "\n",
    "➡️ 𝗘𝗻𝗰𝗼𝗱𝗲𝗿 𝗺𝗼𝗱𝗲𝗹𝘀 work better for \"time series understanding\" tasks—e.g. imputation, anomaly detection.\n",
    "\n",
    "➡️ 𝗘𝗻𝗰𝗼𝗱𝗲𝗿-𝗗𝗲𝗰𝗼𝗱𝗲𝗿 𝗺𝗼𝗱𝗲𝗹𝘀 (e.g. Chronos) remain underexplored. TimeGPT is likely one.\n",
    "\n",
    "This mirrors NLP: encoders for supervised tasks like text classification, decoders for text generation.\n",
    "\n",
    "Btw, a remarkable forecasting model is Toto. Tutorials in the comments! 👇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28c948",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QDRANT_HOST = os.getenv(\"QDRANT_HOST\", \"localhost\")\n",
    "QDRANT_PORT = int(os.getenv(\"QDRANT_PORT\", 6333))\n",
    "OLLAMA_BASE_URL = os.getenv(\"OLLAMA_HOST\", \"localhost\")\n",
    "OLLAMA_PORT = int(os.getenv(\"OLLAMA_PORT\", 11434))\n",
    "DATA_DIR = \"../docs\"\n",
    "REQUIRED_EXTS = [\".txt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bf12f",
   "metadata": {},
   "source": [
    "## Setup the Qdrant vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0402fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name = \"rag_cc\"\n",
    "client = qdrant_client.QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08565489",
   "metadata": {},
   "source": [
    "### Read the documents from a DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6265bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = DATA_DIR\n",
    "loader = SimpleDirectoryReader(\n",
    "    input_dir=DATA_DIR, required_exts=REQUIRED_EXTS, recursive=True\n",
    ")\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c771bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc60e6",
   "metadata": {},
   "source": [
    "## Create an Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa911a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "\n",
    "def create_index(documents):\n",
    "    # Create a QdrantVectorStore instance\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "\n",
    "    # Configure storage settings by specifying the vector store as the storage backend\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # Create an index by embedding each document and storing it in the vector store\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents=documents, storage_context=storage_context\n",
    "    )\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74718db2",
   "metadata": {},
   "source": [
    "### Load the embedding model and index the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b427988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = FastEmbedEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\",\n",
    ")\n",
    "# Add the embedding model to the settings, to be used by the index creation process\n",
    "Settings.embed_model = embed_model\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674eae0",
   "metadata": {},
   "source": [
    "### Load the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f98018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma3n:e2b\", base_url=OLLAMA_BASE_URL, request_timeout=60)\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0bfd15",
   "metadata": {},
   "source": [
    "### Define the Prompt Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2afbf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5784d4",
   "metadata": {},
   "source": [
    "### Reranking \n",
    "- In re-ranking step we use a more sophisticated model (often a cross-encoder) evaluates the initial list of retrieved chunks alongside the query to assign a relevance score to each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be4cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054482c4",
   "metadata": {},
   "source": [
    "### Query the Document "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef9a23",
   "metadata": {},
   "source": [
    "- The query engine integrates the retrieval, re-ranking, and prompt based response generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233cbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10)\n",
    "query_engine.update_prompts({\"response_synthesizer:text_qa_template\": qa_prompt_tmpl})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f96a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"\"\"How did the structure of funding startups in batches contribute to the success and growth of the Y Combinator program and the startups involved?\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d82f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e0c2bf",
   "metadata": {},
   "source": [
    "\n",
    "## Generating Evaluation Dataset using Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41b9df",
   "metadata": {},
   "source": [
    "### Load the knowledge Base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c11718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    \"../docs/paul_graham\",\n",
    ")\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "documents = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ff42fd",
   "metadata": {},
   "source": [
    "### Setting up Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2058a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "generator_llm = ChatOllama(model=\"phi3:3.8b\", base_url=OLLAMA_BASE_URL)\n",
    "critic_llm = ChatOllama(model=\"llama3.2:1b\", base_url=OLLAMA_BASE_URL)\n",
    "ollama_emb = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=OLLAMA_BASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f79a97d",
   "metadata": {},
   "source": [
    "### Creating Ragas Testset Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5032131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
    "# dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602cc86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3bae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = TestsetGenerator.from_langchain(generator_llm, critic_llm, ollama_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e96547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAILS, becaus of some dependency issues with langchain\n",
    "# distribution = {\"simple\":0.6, \"reasoning\":0.3, \"multi_context\":0.245}\n",
    "# testset = generator.generate_with_langchain_docs(documents, testset_size=10, query_distribution=distribution, raise_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbab1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testset from a file\n",
    "# test_df = testset.to_pandas().dropna()\n",
    "test_df = pd.read_csv(\"../docs/paul_graham/test_data_paul_graham.csv\").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7597db4",
   "metadata": {},
   "source": [
    "- Below function that will accept the query engine and a question, and return the answer along with the context it looked at to generate the corresponding answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c170c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query_engine, question):\n",
    "    response = query_engine.query(question)\n",
    "    return {\n",
    "        \"answer\": response.response,\n",
    "        \"context\": [c.node.get_content() for c in response.source_nodes],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e51cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "test_questions = test_df[\"question\"].values\n",
    "\n",
    "responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19134a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, response in enumerate(responses):\n",
    "    print(response.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be51113",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": [response[\"answer\"] for response in responses],\n",
    "    \"contexts\": [response[\"context\"] for response in responses],\n",
    "    \"ground_truth\": test_df[\"ground_truth\"].values.tolist(),\n",
    "}\n",
    "\n",
    "ragas_eval_dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f095a93",
   "metadata": {},
   "source": [
    "## Metric Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d58cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef110e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [faithfulness, answer_correctness, context_recall, context_precision]\n",
    "\n",
    "evaluation_result = evaluate(\n",
    "    llm=critic_llm, embeddings=ollama_emb, dataset=ragas_eval_dataset, metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a147787",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores_df = pd.DataFrame(evaluation_result.scores)\n",
    "eval_scores_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG_CC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
