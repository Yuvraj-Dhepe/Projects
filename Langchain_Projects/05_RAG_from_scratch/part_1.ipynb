{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Asyncio \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Qdrant vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name=\"chat_with_docs\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "input_dir_path = './docs'\n",
    "loader = SimpleDirectoryReader(input_dir=input_dir_path, required_exts=['.pdf'], recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='202660ab-4cff-48ee-a69e-594e98202d1c', embedding=None, metadata={'page_label': '1', 'file_name': 'dspy.pdf', 'file_path': '/home/yuvidh/work/Projects/Langchain_Projects/05_RAG_from_scratch/docs/dspy.pdf', 'file_type': 'application/pdf', 'file_size': 460814, 'creation_date': '2025-01-03', 'last_modified_date': '2024-11-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nDSP Y: C OMPILING DECLARATIVE LANGUAGE\\nMODEL CALLS INTO SELF -IMPROVING PIPELINES\\nOmar Khattab,1 Arnav Singhvi,2\\nParidhi Maheshwari,4 Zhiyuan Zhang,1\\nKeshav Santhanam,1 Sri Vardhamanan,6 Saiful Haq,6\\nAshutosh Sharma,6 Thomas T. Joshi,7 Hanna Moazam,8\\nHeather Miller,3,9 Matei Zaharia,2 Christopher Potts1\\n1Stanford University, 2UC Berkeley, 3Carnegie Mellon University,\\n4Amazon Alexa AI, 5Dashworks Technologies, Inc.,\\n6IIT Bombay, 7Calera Capital, 8Microsoft, 9Two Sigma Investments\\nokhattab@cs.stanford.edu\\nABSTRACT\\nThe ML community is rapidly exploring techniques for prompting language mod-\\nels (LMs) and for stacking them into pipelines that solve complex tasks. Un-\\nfortunately, existing LM pipelines are typically implemented using hard-coded\\n“prompt templates”, i.e. lengthy strings discovered via trial and error. Toward a\\nmore systematic approach for developing and optimizing LM pipelines, we intro-\\nduce DSPy, a programming model that abstracts LM pipelines astext transforma-\\ntion graphs, i.e. imperative computation graphs where LMs are invoked through\\ndeclarative modules. DSPy modules are parameterized, meaning they can learn\\n(by creating and collecting demonstrations) how to apply compositions of prompt-\\ning, finetuning, augmentation, and reasoning techniques. We design a compiler\\nthat will optimize any DSPy pipeline to maximize a given metric. We conduct\\ntwo case studies, showing that succinct DSPy programs can express and optimize\\nsophisticated LM pipelines that reason about math word problems, tackle multi-\\nhop retrieval, answer complex questions, and control agent loops. Within minutes\\nof compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-\\nbootstrap pipelines that outperform standard few-shot prompting (generally by\\nover 25% and 65%, respectively) and pipelines with expert-created demonstra-\\ntions (by up to 5–46% and 16–40%, respectively). On top of that, DSPy pro-\\ngrams compiled to open and relatively small LMs like 770M-parameter T5 and\\nllama2-13b-chat are competitive with approaches that rely on expert-written\\nprompt chains for proprietary GPT-3.5.\\nDSPy is available at https://github.com/stanfordnlp/dspy.\\n1 I NTRODUCTION\\nLanguage models (LMs) are enabling researchers to build NLP systems at higher levels of abstrac-\\ntion and with lower data requirements than ever before (Bommasani et al., 2021). This is fueling an\\nexploding space of “prompting” techniques—and lightweight finetuning techniques—for adapting\\nLMs to new tasks (Kojima et al., 2022), eliciting systematic reasoning from them (Wei et al., 2022;\\nWang et al., 2022b), andaugmenting them with retrieved sources (Guu et al., 2020; Lazaridou et al.,\\n2022; Khattab et al., 2022) or with tools (Yao et al., 2022; Schick et al., 2023). Most of these tech-\\nniques are explored in isolation, but interest has been growing in building multi-stage pipelines and\\nagents that decompose complex tasks into more manageable calls to LMs in an effort to improve\\nperformance (Qi et al., 2019; Khattab et al., 2021a; Karpas et al., 2022; Dohan et al., 2022; Khot\\net al., 2022; Khattab et al., 2022; Chen et al., 2022; Pourreza & Rafiei, 2023; Shinn et al., 2023).\\nUnfortunately, LMs are known to be sensitive to how they are prompted for each task, and this is\\nexacerbated in pipelines where multiple LM calls have to interact effectively. As a result, the LM\\n1\\narXiv:2310.03714v1  [cs.CL]  5 Oct 2023', mimetype=None, path=None, url=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=collection_name)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Embedding Model & Indexing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "# Ensuring same model is used throughout the rag pipeline\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Convert each document into an embedding using the embed model\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model = 'llama3.2:1b', request_timeout=120.0, base_url=\"http://172.18.176.1:11434\")\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Prompt Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "\n",
    "              Query: {query_str}\n",
    "\n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is DSPy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DSPy stands for \"Dependently Specified Programming Yields\", and it's a programming model that abstracts prompting techniques into parameterized declarative modules. This allows developers to define interfaces for their natural language processing (NLP) systems using natural language signatures, which can then be used as prompts or instructions for the LMs (Large Language Models)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(str(response)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
